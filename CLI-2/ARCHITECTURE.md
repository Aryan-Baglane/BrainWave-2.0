# Multi-Agent Architecture Overview

## System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERFACE (CLI)                         â”‚
â”‚                                                                 â”‚
â”‚  Interactive menus powered by Questionary with arrow selection â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚              â”‚
          â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ARCHITECT   â”‚ â”‚ ENGINEER â”‚ â”‚ OBSERVER   â”‚
    â”‚    ğŸ§        â”‚ â”‚   ğŸ‘¨â€ğŸ’»    â”‚ â”‚     ğŸ‘ï¸     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚              â”‚              â”‚
          â”‚              â”‚              â”‚
    Step 1: ANALYZE      â”‚              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚              â”‚
    â”‚ â€¢ Dataset profile  â”‚              â”‚
    â”‚ â€¢ Statistics       â”‚   Step 2: EXECUTE  â”‚
    â”‚ â€¢ Correlations     â”‚              â”‚
    â”‚ â€¢ Missing values   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
    â”‚ â€¢ Outliers         â”‚ â€¢ Imputation â”‚  Step 3: VALIDATE
    â”‚ â€¢ Duplicates       â”‚ â€¢ Encoding   â”‚        â”‚
    â”‚                    â”‚ â€¢ Scaling    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â–º
    â”‚ Creates JSON plan  â”‚ â€¢ Transform  â”‚ â€¢ Verify integrity
    â”‚ with 13+ ops       â”‚              â”‚ â€¢ Check metrics
    â”‚                    â”‚ Returns:     â”‚ â€¢ ML-Readiness
    â”‚                    â”‚ â€¢ df_clean   â”‚ â€¢ Report
    â”‚                    â”‚ â€¢ metrics    â”‚
    â”‚                    â”‚              â”‚ Output:
    â”‚                    â”‚              â”‚ â€¢ User-friendly report
    â”‚                    â”‚              â”‚ â€¢ ML readiness score
    â”‚                    â”‚              â”‚ â€¢ Confidence %
    â”‚                    â”‚              â”‚
    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚    Feedback loop if ML < 95
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
         Dataset â†’ CSV (saved by user)
```

---

## Agent Responsibilities

### ğŸ§  ARCHITECT (Strategic Planner)

**Input**: CSV Dataset  
**Output**: JSON Plan with 13+ operations

**Process**:
1. Call `assess_dataset()` â†’ Dataset metadata
   - Shape, dtypes, missing values %
   - Statistics (mean, std, skewness, kurtosis)
   - Correlation matrix (multicollinearity detection)
   - Cardinality analysis

2. Build detailed context with:
   - Column-by-column analysis
   - Missing value patterns
   - Correlation alerts (> 90%)
   - Data distribution characteristics

3. Call LLM (Llama 3.3-70B) with system prompt:
   - Analyze metadata
   - Identify issues
   - Create logic sequence
   - Confidence scores (0-100)
   - Safety validation

4. Return JSON:
   ```json
   {
     "user_intent_summary": "...",
     "dataset_assessment": {
       "total_rows": 1000,
       "total_columns": 15,
       "ml_readiness_score": 45,
       "critical_issues": [...]
     },
     "recommended_operations": [
       {
         "step": 1,
         "operation": "impute_median",
         "target_columns": ["age"],
         "confidence": 85,
         "reasoning": "...",
         "safety_notes": "..."
       }
     ]
   }
   ```

**Key Features**:
- Detects multicollinearity automatically
- Prevents redundant operations
- Considers ML implications
- Always safe (never drops important columns)

---

### ğŸ‘¨â€ğŸ’» ENGINEER (Executor)

**Input**: DataFrame + JSON Plan  
**Output**: Cleaned DataFrame + Execution metrics

**Supported Operations** (13 types):

| Operation | Effect | Safety |
|-----------|--------|--------|
| `drop` | Remove columns | Checks exist |
| `remove_duplicates` | Remove duplicate rows | Counts removed |
| `impute_median` | Fill nulls with median | Numeric only |
| `impute_mean` | Fill nulls with mean | Numeric only |
| `impute_mode` | Fill nulls with mode | All types |
| `log_transform` | Apply log (handles negatives) | Preserves order |
| `cap_outliers` | IQR-based capping | Robust to extremes |
| `standard_scale` | Mean=0, Std=1 | Sklearn StandardScaler |
| `minmax_scale` | Range [0, 1] | Sklearn MinMaxScaler |
| `one_hot` | Categorical encoding | Creates n-1 columns |
| `label_encode` | Ordinal encoding | For tree models |

**Execution Flow**:
```python
for operation in plan["recommended_operations"]:
    try:
        df = execute_operation(df, operation)
        track_metrics()
    except Exception as e:
        log_error()
        continue

return df, execution_results
```

**Tracking**:
- Before/after shape
- Rows affected per operation
- Total null count change
- Error handling per operation

---

### ğŸ‘ï¸ OBSERVER (Auditor & Reporter)

**Input**: Before DF, After DF, Execution results, Original plan  
**Output**: Validation report + ML-readiness score

**Validation Checks**:
1. **Data Integrity**
   - Is dataset empty?
   - Are critical rows preserved?
   - Target column present?

2. **Transformation Verification**
   - Did nulls decrease?
   - Did operations succeed?
   - Error count = 0?

3. **ML-Readiness Calculation**
   ```
   Base Score: 50
   + 20 if all nulls resolved
   + 15 if all operations succeeded
   - 15 if errors present
   = Final Score (0-100)
   ```

4. **Confidence Calculation**
   ```
   If overall_success: 85%
   Else: 45%
   ```

5. **User-Friendly Report**
   - "What I Did" (natural language)
   - "Dataset Impact" (metrics)
   - "ML Readiness Score" (0-100)
   - "Next Steps" (recommendations)

**Report Example**:
```
ğŸ¯ TRANSFORMATION SUMMARY

What I Did:
  1. Filled 300 missing 'age' values using median
  2. Converted 'gender' to numeric (Female=0, Male=1)
  3. Standardized 'income' and 'credit_score'

Dataset Impact:
  â€¢ Rows: 1,000 â†’ 1,000
  â€¢ Columns: 15 â†’ 17
  â€¢ Null values: 450 â†’ 0

ğŸ“Š ML Readiness: 95/100 âœ“ READY
Confidence: 85%
```

---

## Data Flow Sequence

### Scenario: Titanic Dataset

```
User Loads Dataset
â”‚
â”œâ”€â–º CSV Parse
â”œâ”€â–º 891 rows Ã— 12 columns
â”œâ”€â–º Missing: Age (177), Cabin (687), Embarked (2)
â”‚
â”œâ”€â–º ARCHITECT ANALYSIS
â”œâ”€â–º Statistical summary computed
â”œâ”€â–º Correlations detected
â”œâ”€â–º Plan created:
â”‚   1. impute_median on Age
â”‚   2. drop Cabin (>70% missing)
â”‚   3. impute_mode on Embarked
â”‚   4. one_hot on Sex, Embarked
â”‚   5. standard_scale on Age, Fare
â”‚
â”œâ”€â–º ENGINEER EXECUTION
â”œâ”€â–º df.fillna(df['Age'].median())     âœ”
â”œâ”€â–º df.drop('Cabin', axis=1)           âœ”
â”œâ”€â–º df['Embarked'].fillna(mode)        âœ”
â”œâ”€â–º pd.get_dummies(df[['Sex','Emb']]) âœ”
â”œâ”€â–º StandardScaler().fit_transform()   âœ”
â”‚
â”œâ”€â–º OBSERVER VALIDATION
â”œâ”€â–º Before: 891 Ã— 12, 866 nulls
â”œâ”€â–º After: 891 Ã— 18, 0 nulls
â”œâ”€â–º All operations succeeded âœ“
â”œâ”€â–º No data loss âœ“
â”œâ”€â–º Target preserved âœ“
â”‚
â”œâ”€â–º ML READINESS SCORE
â”œâ”€â–º 50 (base)
â”œâ”€â–º +20 (no nulls)
â”œâ”€â–º +15 (all ops success)
â”œâ”€â–º = 85/100
â”‚
â””â”€â–º User Saves Dataset
    â””â”€â–º cleaned_titanic.csv
```

---

## Memory & Performance

### Dataset Size Handling

| Size | Time | Memory | Approach |
|------|------|--------|----------|
| < 10K rows | < 10s | < 100MB | Direct |
| 10K-100K | 10-60s | 100-500MB | Direct |
| > 100K | 60-600s | > 500MB | Consider chunking |

### Optimization

- Vectorized NumPy operations
- In-place DataFrame modifications
- Generator-based processing (where applicable)
- Scikit-learn efficient scalers

---

## Error Handling

### Graceful Degradation

```python
try:
    result = execute_operation()
except KeyError:
    return {"success": False, "error": "Column not found"}
except ValueError:
    return {"success": False, "error": "Type mismatch"}
except MemoryError:
    return {"success": False, "error": "Dataset too large"}
```

### User Feedback

- âœ” Success (green)
- âš  Warning (yellow)
- âœ– Error (red)
- All errors logged with timestamps

---

## Safety Features

### Constraints (All Agents)

1. **Never drops target column** unless explicit confirmation
2. **Never deletes > 30% data** without user awareness
3. **Always preserves column names** in mapping
4. **Validates before executing** (dry-run logic)
5. **Logs everything** (timestamps, decisions, assumptions)
6. **Fails gracefully** (errors don't crash system)

### Validation Gates

- âœ… Dataset not empty
- âœ… Nulls reduced (or operations explain why)
- âœ… All transformations reversible (via logging)
- âœ… Target column present and clean
- âœ… No accidental type conversions

---

## Configuration

### Environment Variables

```env
GROQ_API_KEY=sk-...                    # Required
GROQ_MODEL=llama-3.3-70b-versatile     # Optional
LOG_LEVEL=INFO                          # Optional
LLM_TEMPERATURE=0.1                     # Optional
LLM_MAX_TOKENS=2000                     # Optional
```

### Customization Points

1. **Add new operations**: Extend `Engineer._execute_operation()`
2. **Change scoring**: Modify `Observer` calculation
3. **Adjust temperature**: Higher = more creative (risky)
4. **Change model**: Use different Groq model

---

## Dependencies

```
pandas>=1.5.0              # Data manipulation
numpy>=1.23.0              # Numerical computing
scikit-learn>=1.2.0        # Preprocessing, scaling
groq>=0.11.0               # LLM API
questionary>=1.10.0        # Interactive menus
rich>=13.0.0               # Terminal formatting
python-dotenv>=1.0.0       # Environment management
scipy>=1.10.0              # Statistical functions
```

---

## Next Steps

1. **Install**: `pip install -r requirements.txt`
2. **Configure**: Copy `.env.example` â†’ `.env`, add API key
3. **Run**: `python cli.py`
4. **Load Dataset**: Select CSV file
5. **Analyze**: Let Architect create plan
6. **Execute**: Let Engineer transform data
7. **Save**: Export cleaned dataset

---

## Version History

- **v4.0.0** (Jan 2026): Multi-agent architecture, full integration
- **v3.3.0** (Previous): Single-agent system

---

## Support & Debugging

- All operations logged to console
- Timestamps on all messages
- Stack traces on errors
- Validation reports after execution
- ML-readiness breakdown

---

**Built with â¤ï¸ using Groq API + Advanced Multi-Agent Architecture**
