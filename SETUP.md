# AI Data Engineering System - Setup Guide

## Overview
This is a production-grade multi-agent data cleaning system powered by Groq's Llama 3.3-70B model.

**Features:**
- üß† **Architect Agent**: Analyzes datasets and creates intelligent cleaning plans
- üë®‚Äçüíª **Engineer Agent**: Safely executes data transformations
- üëÅÔ∏è **Observer Agent**: Validates results and provides ML-readiness scores
- üîí **Secure**: Environment-based API key management
- üìä **Smart**: Correlation detection, multicollinearity checks, ML-ready validation

---

## Installation

### 1. Clone/Setup Repository
```bash
cd d:\aadhar
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Setup Environment Variables
```bash
# Copy the example file
copy .env.example .env

# Edit .env and add your Groq API key
# Get your key from: https://console.groq.com/keys
```

**Example .env:**
```
GROQ_API_KEY=your_actual_api_key_here
GROQ_MODEL=llama-3.3-70b-versatile
LOG_LEVEL=INFO
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2000
```

### 4. Run the System
```bash
python cli.py
```

---

## How It Works

### The Three-Agent Architecture

#### 1. **Architect Agent** üß†
- **Role**: Senior Data Scientist
- **Tasks**:
  - Analyzes dataset metadata (shape, dtypes, nulls, statistics)
  - Detects correlation issues (multicollinearity > 90%)
  - Creates comprehensive cleaning plans
  - Provides ML-readiness score predictions

- **Process**:
  ```
  Dataset ‚Üí Analysis ‚Üí Context Building ‚Üí LLM Plan Generation
  ```

#### 2. **Engineer Agent** üë®‚Äçüíª
- **Role**: ML Engineer
- **Tasks**:
  - Executes all data transformations safely
  - Handles 13+ operation types (imputation, encoding, scaling, etc.)
  - Tracks before/after metrics
  - Provides detailed execution logs

- **Supported Operations**:
  - `impute_median` / `impute_mean` / `impute_mode`
  - `drop` - Drop columns
  - `remove_duplicates`
  - `standard_scale` / `minmax_scale`
  - `log_transform`
  - `cap_outliers` - IQR-based outlier handling
  - `one_hot` - One-hot encoding
  - `label_encode` - Label encoding

#### 3. **Observer Agent** üëÅÔ∏è
- **Role**: Data Quality Auditor
- **Tasks**:
  - Validates transformation success
  - Checks data integrity
  - Calculates ML-readiness score
  - Generates user-friendly reports
  - Provides confidence metrics

- **ML-Readiness Scoring**:
  - Base: 50/100
  - +20 if no nulls remaining
  - +15 if all operations succeeded
  - -15 if errors occurred

---

## Workflow

### Step 1: Load Dataset
```
Main Menu ‚Üí Load Dataset ‚Üí Enter CSV path
```
Supports any CSV file. Example:
```
d:\aadhar\titanic.csv
```

### Step 2: Analyze & Create Plan
```
Main Menu ‚Üí Analyze & Create Plan ‚Üí Describe your goal
```
System will:
1. Architect analyzes the dataset
2. LLM generates intelligent transformation plan
3. Shows you the plan before execution

### Step 3: Execute Plan
```
Main Menu ‚Üí Execute Plan
```
System will:
1. Engineer executes all operations safely
2. Observer validates each step
3. Shows you detailed before/after metrics
4. Calculates ML-readiness score

### Step 4: Save Dataset
```
Main Menu ‚Üí Save Dataset ‚Üí Enter filename
```
Default: `cleaned_<original_filename>`

---

## System Features

### Smart Correlation Detection
```
‚ö†Ô∏è CORRELATION ALERTS (Multicollinearity)
  ‚Ä¢ feature1 correlates with [feature2, feature3]
  ‚Ä¢ correlation > 90% detected
  ‚Üí Architect recommends dropping redundant columns
```

### ML-Readiness Score
```
üìä ML Readiness: 95/100 ‚úì READY

‚úì Missing values: Resolved (450 ‚Üí 0)
‚úì Categorical encoding: Complete (3 columns)
‚úì Scaling: Applied to 4 numeric features
‚úì Target column: Clean and present
```

### Safety Guardrails
- ‚úÖ Never drops target column without confirmation
- ‚úÖ Validates data integrity after each operation
- ‚úÖ Logs all transformations with timestamps
- ‚úÖ Preserves original data until explicit save
- ‚úÖ Handles errors gracefully with recovery suggestions

---

## Example Usage

### Scenario: Clean Titanic Dataset

```bash
python cli.py
```

**Output:**
```
üîí Enter Groq API Key: ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢
‚úî Authentication Successful!

‚öô MAIN MENU
> Load Dataset

Path to CSV file > d:\aadhar\titanic.csv
‚úî Loaded: titanic.csv
Shape: 891 rows √ó 12 columns

‚öô MAIN MENU
> Analyze & Create Plan

Describe your data preparation goal: Prepare for ML classification
üß† Architect analyzing dataset...

üìã RECOMMENDED OPERATIONS:
  1. impute_median on ['Age']
  2. one_hot on ['Sex', 'Embarked']
  3. standard_scale on ['Age', 'Fare']

‚öô MAIN MENU
> Execute Plan

üë®‚Äçüíª Engineer executing operations...
‚úî Step 1: Imputed 1 columns with median
‚úî Step 2: One-hot encoded 2 categorical columns
‚úî Step 3: Standard scaled 2 columns (mean‚âà0, std‚âà1)

üëÅÔ∏è Observer validating results...

üéØ TRANSFORMATION SUMMARY

What I Did:
  1. Imputed 1 columns with median
  2. One-hot encoded 2 categorical columns
  3. Standard scaled 2 columns (mean‚âà0, std‚âà1)

Dataset Impact:
  ‚Ä¢ Rows: 891 ‚Üí 891
  ‚Ä¢ Columns: 12 ‚Üí 18
  ‚Ä¢ Null values: 177 ‚Üí 0

üìä ML Readiness: 97/100 ‚úì READY
Confidence: 85%

‚úî DATASET IS ML-READY!

‚öô MAIN MENU
> Save Dataset

Output filename > cleaned_titanic.csv
‚úî Saved to cleaned_titanic.csv
```

---

## Troubleshooting

### "API Key Required. Exiting."
- Set `GROQ_API_KEY` in your `.env` file
- Or enter it when prompted

### "File not found. Try again."
- Use absolute path (e.g., `d:\path\to\file.csv`)
- Use forward slashes or double backslashes
- Verify file exists

### "Architect failed"
- Check API rate limits
- Verify internet connection
- Check Groq API status

### "Operation failed"
- Check if column names are correct
- Verify data types match operation requirements
- Try Preview Data to inspect dataset

---

## Configuration Options

Edit `.env` to customize behavior:

```env
# Required
GROQ_API_KEY=sk-...

# Optional
GROQ_MODEL=llama-3.3-70b-versatile
LOG_LEVEL=INFO              # DEBUG, INFO, WARNING, ERROR
LLM_TEMPERATURE=0.1         # 0.0 (deterministic) to 2.0 (creative)
LLM_MAX_TOKENS=2000         # Max response length
```

**Temperature Guide:**
- `0.1` (default): Precise, deterministic (recommended for data cleaning)
- `0.5`: Balanced
- `1.0`: More creative

---

## API Reference

### Main Classes

#### `Architect`
```python
architect = Architect(client, model)

# Assess dataset
analysis = architect.assess_dataset(df, "titanic.csv")

# Create plan
plan = architect.create_plan(analysis, "Prepare for ML")
```

#### `Engineer`
```python
engineer = Engineer()

# Execute plan
df_clean, results = engineer.execute_plan(df, plan)
```

#### `Observer`
```python
observer = Observer()

# Validate
validation = observer.validate_execution(df_before, df_after, results, plan)

# Generate report
report = observer.generate_report(results, validation, plan)
```

---

## Best Practices

1. **Start Simple**: Begin with small datasets to test
2. **Preview First**: Use "Preview Data" before executing
3. **Check Correlations**: Review the correlation alerts
4. **Read Reasoning**: Understand why each operation is recommended
5. **Iterate**: If ML readiness < 95, run another cycle
6. **Backup Original**: Keep original data file separate

---

## Supported Data Types

| Type | Operations |
|------|-----------|
| Numeric | Scaling, log transform, imputation, outlier capping |
| Categorical | One-hot, label encoding, imputation (mode) |
| Mixed | Duplicate removal, correlation analysis |

---

## Performance

- **Small datasets** (< 10K rows): < 30 seconds per cycle
- **Medium datasets** (10K-100K rows): 30-120 seconds per cycle
- **Large datasets** (> 100K rows): May require chunking

---

## Support

- Check logs: `logging` module writes detailed logs
- Review code: All agent logic is transparent and commented
- Modify operations: Edit `Engineer._execute_operation()` to customize

---

## License & Credits

**Built with:**
- Groq Cloud (Llama 3.3-70B model)
- Pandas, NumPy, Scikit-learn
- Rich (CLI formatting)
- Questionary (interactive menus)

**Version:** 4.0.0
**Last Updated:** January 16, 2026

---

## Quick Start (TL;DR)

```bash
# 1. Install
pip install -r requirements.txt

# 2. Setup
copy .env.example .env
# Edit .env and add GROQ_API_KEY

# 3. Run
python cli.py

# 4. Load ‚Üí Analyze ‚Üí Execute ‚Üí Save
```

Enjoy your super-intelligent data cleaning! üöÄ
